{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pyton to learn A perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting assignment1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile assignment1.py\n",
    "#%load assignment1.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def learn_perceptron(neg_examples_nobias, pos_examples_nobias, w_init, w_gen_feas):\n",
    "    ''' Learns the weights of a perceptron and displays the results.\n",
    "    Learns the weights of a perceptron for a 2-dimensional dataset and plots\n",
    "    the perceptron at each iteration where an iteration is defined as one\n",
    "    full pass through the data. If a generously feasible weight vector\n",
    "    is provided then the visualization will also show the distance\n",
    "    of the learned weight vectors to the generously feasible weight vector.\n",
    "    Required Inputs:\n",
    "        neg_examples_nobias - The num_neg_examples x 2 matrix for the\n",
    "            examples with target 0.\n",
    "        num_neg_examples is the number of examples for the negative class.\n",
    "        pos_examples_nobias - The num_pos_examples x 2 matrix for the\n",
    "            examples with target 1.\n",
    "        num_pos_examples is the number of examples for the positive class.\n",
    "        w_init - A 3-dimensional initial weight vector. The last element\n",
    "            is the bias.\n",
    "        w_gen_feas - A generously feasible weight vector.\n",
    "    Returns:\n",
    "        w - The learned weight vector.\n",
    "    '''\n",
    "\n",
    "    # Bookkeeping\n",
    "    num_neg_examples = len(neg_examples_nobias)\n",
    "    num_pos_examples = len(pos_examples_nobias)\n",
    "    num_err_history = []\n",
    "    w_dist_history = []\n",
    "\n",
    "    # Here we add a column of ones to the examples in order to allow us to learn\n",
    "    # bias parameters.\n",
    "    neg_examples = np.append(neg_examples_nobias,\n",
    "                             np.ones((num_neg_examples,1), dtype=float),\n",
    "                             axis=1)\n",
    "    pos_examples = np.append(pos_examples_nobias,\n",
    "                             np.ones((num_pos_examples,1), dtype=float),\n",
    "                             axis=1)\n",
    "\n",
    "    # Find the data points that the perceptron has incorrectly classified\n",
    "    # and record the number of errors it makes.\n",
    "    iter = 0\n",
    "    w = w_init\n",
    "    [mistakes0, mistakes1] = eval_perceptron(neg_examples,pos_examples,w)\n",
    "    num_errs = len(mistakes0) + len(mistakes1)\n",
    "    num_err_history.append(num_errs)\n",
    "\n",
    "    print('Number of errors in iteration', iter, ':', num_errs)\n",
    "    print('Weights:', w.tolist())\n",
    "\n",
    "    plot_perceptron(neg_examples, pos_examples, mistakes0, mistakes1,\n",
    "                    num_err_history, w, w_dist_history)\n",
    "    # input('Press Enter to continue...')\n",
    "\n",
    "    # If a generously feasible weight vector exists, record the distance\n",
    "    # to it from the initial weight vector.\n",
    "\n",
    "    if len(w_gen_feas) > 0:\n",
    "        w_dist_history.append(np.linalg.norm(w - w_gen_feas))\n",
    "\n",
    "    # Iterate until the perceptron has correctly classified all points.\n",
    "    while num_errs > 0:\n",
    "        iter = iter + 1\n",
    "\n",
    "        # Update the weights of the perceptron.\n",
    "        w = update_weights(neg_examples,pos_examples,w)\n",
    "\n",
    "        # If a generously feasible weight vector exists, record the distance\n",
    "        # to it from the current weight vector.\n",
    "        if len(w_gen_feas) > 0:\n",
    "            w_dist_history.append(np.linalg.norm(w - w_gen_feas))\n",
    "\n",
    "        # Find the data points that the perceptron has incorrectly classified.\n",
    "        # and record the number of errors it makes.\n",
    "        [mistakes0, mistakes1] = eval_perceptron(neg_examples,pos_examples,w)\n",
    "        num_errs = len(mistakes0) + len(mistakes1)\n",
    "        num_err_history.append(num_errs)\n",
    "\n",
    "        print('Number of errors in iteration', iter, ':', num_errs)\n",
    "        print('Weights:', w.tolist())\n",
    "\n",
    "        plot_perceptron(neg_examples, pos_examples, mistakes0, mistakes1,\n",
    "                        num_err_history, w, w_dist_history)\n",
    "        # input('<Press enter to continue...',)\n",
    "    return w\n",
    "\n",
    "# WRITE THE CODE TO COMPLETE THIS FUNCTION\n",
    "def update_weights(neg_examples, pos_examples, w_current):\n",
    "    ''' Updates the weights of the perceptron for incorrectly classified points\n",
    "        using the perceptron update algorithm. This function makes one sweep\n",
    "        over the dataset.\n",
    "    Inputs:\n",
    "        neg_examples - The num_neg_examples x 3 matrix for the examples\n",
    "            with target 0.\n",
    "            num_neg_examples is the number of examples for the negative class.\n",
    "        pos_examples - The num_pos_examples x 3 matrix for the examples\n",
    "            with target 1.\n",
    "            num_pos_examples is the number of examples for the positive class.\n",
    "        w_current - A 3-dimensional weight vector, the last element is\n",
    "            the bias.\n",
    "    Returns:\n",
    "        w - The weight vector after one pass through the dataset using\n",
    "            the perceptron learning rule.\n",
    "    '''\n",
    "    w = w_current\n",
    "    for i in range(neg_examples.shape[0]):\n",
    "        this_case = neg_examples[i]\n",
    "        activation = this_case.dot(w)[0]\n",
    "        if (activation >= 0):\n",
    "            #WRITE YOUR CODE HERE \n",
    "            learning_rate = 1\n",
    "            target = 0\n",
    "            w = np.add(w , (learning_rate * ((target - activation) * this_case)).reshape(-1, 1))\n",
    "\n",
    "    for i in range(pos_examples.shape[0]):\n",
    "        this_case = pos_examples[i]\n",
    "        activation = this_case.dot(w)[0]\n",
    "        if (activation < 0):\n",
    "            #WRITE YOUR CODE HERE\n",
    "            learning_rate = 1\n",
    "            target = 1\n",
    "            w = np.add(w , (learning_rate * ((target - activation) * this_case)).reshape(-1, 1))\n",
    "\n",
    "    return w\n",
    "\n",
    "def eval_perceptron(neg_examples, pos_examples, w):\n",
    "    ''' Evaluates the perceptron using a given weight vector.\n",
    "    Here, evaluation refers to finding the data points that the\n",
    "    perceptron incorrectly classifies.\n",
    "    Inputs:\n",
    "        neg_examples - The num_neg_examples x 3 matrix for the examples\n",
    "            with target 0.\n",
    "            num_neg_examples is the number of examples for the negative class.\n",
    "        pos_examples - The num_pos_examples x 3 matrix for the examples\n",
    "            with target 1.\n",
    "            num_pos_examples is the number of examples for the positive class.\n",
    "        w - A 3-dimensional weight vector, the last element is the bias.\n",
    "    Returns:\n",
    "        mistakes0 - A vector containing the indices of the negative examples\n",
    "            that have been incorrectly classified as positive.\n",
    "        mistakes1 - A vector containing the indices of the positive examples\n",
    "            that have been incorrectly classified as negative.\n",
    "    '''\n",
    "\n",
    "    num_neg_examples = neg_examples.shape[0]\n",
    "    num_pos_examples = pos_examples.shape[0]\n",
    "    mistakes0 = []\n",
    "    mistakes1 = []\n",
    "\n",
    "    for i in range(num_neg_examples):\n",
    "        x = neg_examples[i]\n",
    "        activation = x.dot(w)[0]\n",
    "        if activation >= 0:\n",
    "            mistakes0.append(i)\n",
    "\n",
    "    for i in range(num_pos_examples):\n",
    "        x = pos_examples[i]\n",
    "        activation = x.dot(w)[0]\n",
    "        if activation < 0:\n",
    "            mistakes1.append(i)\n",
    "\n",
    "    return [mistakes0, mistakes1]\n",
    "\n",
    "def plot_perceptron(neg_examples, pos_examples, mistakes0, mistakes1,\n",
    "                    num_err_history, w, w_dist_history):\n",
    "    ''' Plots information about a perceptron classifier on a 2-dimensional dataset.\n",
    "    The top-left plot shows the dataset and the classification boundary given by\n",
    "    the weights of the perceptron. The negative examples are shown as circles\n",
    "    while the positive examples are shown as squares. If an example is colored\n",
    "    green then it means that the example has been correctly classified by the\n",
    "    provided weights. If it is colored red then it has been incorrectly classified.\n",
    "    The top-right plot shows the number of mistakes the perceptron algorithm has\n",
    "    made in each iteration so far.\n",
    "    The bottom-left plot shows the distance to some generously feasible weight\n",
    "    vector if one has been provided (note, there can be an infinite number of these).\n",
    "    Points that the classifier has made a mistake on are shown in red,\n",
    "    while points that are correctly classified are shown in green.\n",
    "    The goal is for all of the points to be green (if it is possible to do so).\n",
    "    Inputs:\n",
    "        neg_examples - The num_neg_examples x 3 matrix for the examples\n",
    "            with target 0.\n",
    "            num_neg_examples is the number of examples for the negative class.\n",
    "        pos_examples- The num_pos_examples x 3 matrix for the examples\n",
    "            with target 1.\n",
    "            num_pos_examples is the number of examples for the positive class.\n",
    "        mistakes0 - A vector containing the indices of the datapoints from\n",
    "            class 0 incorrectly classified by the perceptron. This is a\n",
    "            subset of neg_examples.\n",
    "        mistakes1 - A vector containing the indices of the datapoints from\n",
    "            class 1 incorrectly classified by the perceptron. This is a\n",
    "            subset of pos_examples.\n",
    "        num_err_history - A vector containing the number of mistakes for each\n",
    "            iteration of learning so far.\n",
    "        w - A 3-dimensional vector corresponding to the current weights of the\n",
    "            perceptron. The last element is the bias.\n",
    "        w_dist_history - A vector containing the L2-distance to a generously\n",
    "            feasible weight vector for each iteration of learning so far.\n",
    "            Empty if one has not been provided.\n",
    "    '''\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "\n",
    "    neg_examples_right = []\n",
    "    neg_examples_mistakes = []\n",
    "    pos_examples_right = []\n",
    "    pos_examples_mistakes = []\n",
    "\n",
    "    for i,k in enumerate(neg_examples):\n",
    "        if i in mistakes0:\n",
    "            neg_examples_mistakes.append([k[0],k[1]])\n",
    "        else:\n",
    "            neg_examples_right.append([k[0],k[1]])\n",
    "\n",
    "    for i,k in enumerate(pos_examples):\n",
    "        if i in mistakes1:\n",
    "            pos_examples_mistakes.append([k[0],k[1]])\n",
    "        else:\n",
    "            pos_examples_right.append([k[0],k[1]])\n",
    "\n",
    "    plt.subplot(221)\n",
    "    if len(neg_examples_right)>0:\n",
    "        plt.plot(np.array(neg_examples_right)[:,0],\n",
    "                 np.array(neg_examples_right)[:,1], 'go')\n",
    "    if len(pos_examples_right)>0:\n",
    "        plt.plot(np.array(pos_examples_right)[:,0],\n",
    "                 np.array(pos_examples_right)[:,1], 'gs')\n",
    "    if len(neg_examples_mistakes)>0:\n",
    "        plt.plot(np.array(neg_examples_mistakes)[:,0],\n",
    "                 np.array(neg_examples_mistakes)[:,1], 'ro')\n",
    "    if len(pos_examples_mistakes)>0:\n",
    "        plt.plot(np.array(pos_examples_mistakes)[:,0],\n",
    "                 np.array(pos_examples_mistakes)[:,1], 'rs')\n",
    "\n",
    "    plt.title('Classifier')\n",
    "\n",
    "    # In order to plot the decision line, we just need to get two points.\n",
    "    plt.plot([-5,5],[(-w[-1]+5*w[0])/w[1],(-w[-1]-5*w[0])/w[1]],'k')\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "\n",
    "    plt.subplot(222)\n",
    "    # print(num_err_history)\n",
    "    plt.plot(range(len(num_err_history)),num_err_history,'ok')\n",
    "    plt.xlim([-1,max(15,len(num_err_history))])\n",
    "    plt.ylim([0,len(neg_examples)+len(pos_examples)+1])\n",
    "    plt.title('Number of errors')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Number of errors')\n",
    "    #\n",
    "    plt.subplot(2,2,3)\n",
    "    if len(w_dist_history)>0:\n",
    "        plt.plot(range(len(w_dist_history)),w_dist_history,'ok')\n",
    "        plt.xlim([-1,max(15,len(num_err_history))])\n",
    "        plt.ylim([0,15])\n",
    "        plt.title('Distance')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Distance')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#### Main program\n",
    "\n",
    "## Loading data\n",
    "data1 = sio.loadmat('dataset3.mat')\n",
    "neg_examples_nobias = data1['neg_examples_nobias']\n",
    "pos_examples_nobias = data1['pos_examples_nobias']\n",
    "\n",
    "# If weight vectors have not been provided, initialize them appropriately.\n",
    "# Else just read them.\n",
    "if ('w_init' in data1) and len(data1['w_init'])>0:\n",
    "    w_init = data1['w_init']\n",
    "else:\n",
    "    w_init = np.random.randn(3).reshape(3,1)\n",
    "\n",
    "if 'w_gen_feas' in data1:\n",
    "    w_gen_feas = data1['w_gen_feas']\n",
    "else:\n",
    "    w_gen_feas = []\n",
    "\n",
    "## Launch learning\n",
    "learn_perceptron(neg_examples_nobias, pos_examples_nobias, w_init, w_gen_feas)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
